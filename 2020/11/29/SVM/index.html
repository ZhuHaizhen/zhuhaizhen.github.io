<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>SVM算法 | Haizhen's Blog</title><meta name="description" content="支持向量机的理解与推导，如有错漏恳请指出"><meta name="keywords" content="machine learning,SVM"><meta name="author" content="Zhu Haizhen,zhuhaizhen1024@163.com"><meta name="copyright" content="Zhu Haizhen"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/image/favicon.png"><link rel="canonical" href="https://zhuhaizhen.github.io/2020/11/29/SVM/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="SVM算法"><meta property="og:url" content="https://zhuhaizhen.github.io/2020/11/29/SVM/"><meta property="og:site_name" content="Haizhen's Blog"><meta property="og:description" content="支持向量机的理解与推导，如有错漏恳请指出"><meta property="og:image" content="https://s1.ax1x.com/2022/04/02/qo98Fe.jpg"><meta property="article:published_time" content="2020-11-29T03:36:29.000Z"><meta property="article:modified_time" content="2022-04-05T12:25:34.678Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="prev" title="SMO算法" href="https://zhuhaizhen.github.io/2020/12/05/SMO/"><link rel="next" title="机器学习揭示微生物生态网络中丢失的边及潜在的相互作用机制" href="https://zhuhaizhen.github.io/2020/11/21/1121/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6b1c40d9cb0aeba42b229c550f73967b";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"ULAMU2VKLC","apiKey":"074084189662d00e3b579aef488f16a5","indexName":"hexo-blog","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: {"languages":{"author":"作者: Zhu Haizhen","link":"链接: ","source":"来源: Haizhen's Blog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/image/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">25</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">35</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">5</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/collection/"><i class="fa-fw fas fa-paw"></i><span> 随手记</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comments"></i><span> 留言板</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hard-margin-SVM"><span class="toc-number">1.</span> <span class="toc-text">Hard margin SVM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Soft-margin-SVM"><span class="toc-number">2.</span> <span class="toc-text">Soft margin SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#合页损失函数-Hinge-lost"><span class="toc-number">2.1.</span> <span class="toc-text">合页损失函数 (Hinge lost)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kernel-SVM"><span class="toc-number">3.</span> <span class="toc-text">Kernel SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#常用的核函数"><span class="toc-number">3.1.</span> <span class="toc-text">常用的核函数</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://s1.ax1x.com/2022/04/02/qo98Fe.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Haizhen's Blog</a></span><span class="pull-right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/collection/"><i class="fa-fw fas fa-paw"></i><span> 随手记</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comments"></i><span> 留言板</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">SVM算法</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-11-29 11:36:29"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2020-11-29</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2022-04-05 20:25:34"><i class="fas fa-history fa-fw"></i> 更新于 2022-04-05</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/%E7%AE%97%E6%B3%95%E7%90%86%E8%A7%A3/">算法理解</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta__icon"></i><span>字数总计:</span><span class="word-count">2.5k</span><span class="post-meta__separator">|</span><i class="far fa-clock fa-fw post-meta__icon"></i><span>阅读时长: 11 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="far fa-comments fa-fw post-meta__icon"></i><span>评论数:</span><a href="/2020/11/29/SVM/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/2020/11/29/SVM/" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h2 id="Hard-margin-SVM"><a href="#Hard-margin-SVM" class="headerlink" title="Hard margin SVM"></a>Hard margin SVM</h2><p>支持向量机 (support vector machine, SVM)是一种<strong>二分类算法</strong>，其基础模型是在空间中寻找一条分界线，使得两个类别之间的<strong>间隔最大</strong>，因此也叫作最大间隔分类器 (maximum margin classifier)</p>
<p><img src= "/img/loading.gif" data-src="https://s1.ax1x.com/2022/04/01/qh0cAf.png" alt=""></p>
<ul>
<li><p>SVM的决策边界 (decision boundary)可以定义为一个线性方程：$w^TX + b = 0$，其中：</p>
<ul>
<li><p>$w = {\lbrace w_1,w_2,…,w_n\rbrace}$ 是一个法向量，n是特征的个数</p>
</li>
<li><p>$X$ 是训练数据</p>
</li>
<li><p>$b$ 是偏置，决定决策边界与原点之间的距离</p>
</li>
</ul>
</li>
<li><p>SVM用数学公式表示如下：<br>$$\begin{cases}<br>w^Tx_i + b &gt; 0, \ y_i = 1 \\<br>w^Tx_i + b &lt; 0, \ y_i = -1<br>\end{cases}$$</p>
</li>
</ul>
<blockquote>
<p>将两个式子合并即为：$y_i(w^Tx_i + b) &gt; 0,\ i \in {\lbrace 1,2,…,m \rbrace}$</p>
</blockquote>
<ul>
<li><p>两个类别的训练数据中距离决策边界最近的数据称为<strong>支持向量</strong> (support vector)，它们分别位于两条平行于决策边界的线，满足：$w^Tx_i + b = 1$ 或 $w^Tx_i + b = -1$，由此可以推导两个类别之间的间隔：<br>$$\begin{align}<br>&amp; \because w^Tx_1 + b = 1 \\<br>&amp; \quad\ w^Tx_2 +b = -1 \\<br>&amp; \therefore (w^Tx_1 + b) - (w^Tx_2 + b) = 2 \\<br>&amp; \quad\ w^T(x_1 - x_2) = 2 \\<br>&amp; \quad\ w^T(x_1 - x_2) = ||w||_2||x_1 - x_2||_2cos\theta = 2 \\<br>&amp; \quad\ d_1 + d_2 = {||x_1 - x_2||}_2cos\theta = \frac {2}{ {||w||}_2}<br>\end{align}$$</p>
</li>
<li><p>由此可得SVM的目标函数 (objective function)：<br>$$\min_{w,b} \frac {1}{2}{||w||_2}^2 \qquad s.t. \ y_i(w^Tx_i + b) \geq 1, \ i \in \lbrace 1,2,…,m\rbrace$$</p>
<blockquote>
<p>约束条件可以写为：$1 - y_i(w^Tx_i + b) \leq 0$</p>
</blockquote>
</li>
<li><p>将优化问题添加拉格朗日算子：<br>$$L(w,b,\alpha) = \frac {1}{2}{||w||<em>2}^2 + \sum</em>{i=1}^m\alpha_i[1 - y_i(w^Tx_i + b)] \qquad s.t. \ \alpha_i \geq 0, \ i \in \lbrace1,2,…,m\rbrace$$</p>
</li>
<li><p>原优化问题 (primal problem)等价于：<br>$$p^* = \min_{w,b}\max_{\alpha \geq 0} \ L(w,b,\alpha)$$</p>
</li>
<li><p>原问题的对偶问题 (dual problem)为：<br>$$d^* = \max_{\alpha \geq 0}\min_{w,b} \ L(w,b,\alpha)$$</p>
</li>
<li><p>要满足 $p^* = d^*$ (strong duality)，需满足：</p>
<ul>
<li><p>优化问题是凸优化 (convex)问题，约束函数是仿射函数 (affine)</p>
</li>
<li><p>满足KKT (Karush-Kuhn-Tucker)条件，即：</p>
<ol>
<li><p>$\frac {\partial L}{\partial w} = 0, \ \frac {\partial L}{\partial b} = 0, \ \frac {\partial L}{\partial \alpha} = 0$</p>
</li>
<li><p>$\alpha_i[1 - y_i(w^Tx_i + b)] = 0$</p>
<blockquote>
<p>互补松弛条件，slackness complementary</p>
</blockquote>
</li>
<li><p>$\alpha_i \geq 0$</p>
</li>
<li><p>$1 - y_i(w^Tx_i + b) \leq 0$</p>
</li>
</ol>
</li>
</ul>
</li>
<li><p>令 $\frac {\partial L}{\partial w} = 0$，可得：<br>$$\begin{aligned}<br>&amp; \frac {\partial L}{\partial w} = w - \sum_{i=1}^m\alpha_iy_ix_i = 0 \\<br>&amp; w^* = \sum_{i=1}^m\alpha_iy_ix_i<br>\end{aligned}$$</p>
</li>
<li><p>令 $\frac {\partial L}{\partial b} = 0$，可得：<br>$$\begin{aligned}<br>\frac {\partial L}{\partial b} = -\sum_{i=1}^m\alpha_iy_i = 0<br>\end{aligned}$$</p>
</li>
<li><p>将上述两个式子带入拉格朗日函数，消去 $w$ 和 $b$，求得 $\min_{w,b} \ L(w,b,\alpha)$：<br>$$\begin{aligned}<br>&amp; L(\alpha) = \frac {1}{2} \sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) - \sum_{i=1}^m\alpha_iy_i[\sum_{j=1}^m(\alpha_jy_jx_j) \cdot x_j + b] + \sum_{i=1}^m\alpha_i \\<br>&amp; \qquad \ = -\frac {1}{2} \sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) + \sum_{i=1}^m\alpha_i<br>\end{aligned}$$</p>
</li>
<li><p>原问题转为：<br>$$\max_\alpha \ -\frac {1}{2} \sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) + \sum_{i=1}^m\alpha_i \quad s.t. \sum_{i=1}^m\alpha_iy_i = 0, \ \alpha_i \geq 0, \ i \in \lbrace 1,2,…,m\rbrace$$</p>
</li>
<li><p>将上面的式子加负号，变成最小化问题：<br>$$\min_\alpha \ \frac {1}{2} \sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) - \sum_{i=1}^m\alpha_i \quad s.t. \sum_{i=1}^m\alpha_iy_i = 0, \ \alpha_i \geq 0, \ i \in \lbrace 1,2,…,m\rbrace$$</p>
<blockquote>
<p>通过序列最小优化算法 (sequential minimal optimiztion, SMO)可以求解 $\alpha$</p>
</blockquote>
</li>
<li><p>对任意训练样本 $(x_i,y_i)$，总有 $\alpha_i = 0$ 或者 $y_i(w^Tx_i + b) = 1$</p>
<ul>
<li><p>若 $\alpha_i = 0$，则该训练样本不必在最终的优化问题中进行计算</p>
</li>
<li><p>若 $\alpha_i &gt; 0$，则 $y_i(w^Tx_i + b) = 1$，该训练样本为支持向量</p>
</li>
</ul>
</li>
</ul>
<p><strong>支持向量机的重要性质：训练完成后，大部分训练数据不需要保留，最终的模型只与支持向量有关</strong></p>
<h2 id="Soft-margin-SVM"><a href="#Soft-margin-SVM" class="headerlink" title="Soft margin SVM"></a>Soft margin SVM</h2><p>Hard margin SVM适用于训练数据完全线性可分的情况，但在实际情况下，训练数据一般存在一些异常值，使得两个类别并不完全线性可分。Soft margin SVM引入一个松弛变量 $\xi_i$ ，用来表征第 $i$ 个训练样本偏离正确分类的程度。同时引入一个超参数 (hyperparameter) $C$ ，用来控制对这些异常点进行惩罚的程度</p>
<p><img src= "/img/loading.gif" data-src="https://s1.ax1x.com/2022/04/01/qh024S.png" alt=""></p>
<ul>
<li><p>带松弛变量的目标函数：<br>$$\min_{w,b,\xi} ; \frac {1}{2}{||w||<em>2}^2 + C\sum</em>{i=1}^m\xi_i \quad s.t. \ y_i(w^Tx_i + b) \geq 1 - \xi_i, \ C &gt; 0, \ \xi_i \geq 0, \ i \in \lbrace 1,2,…,m\rbrace$$</p>
<blockquote>
<p>不等式约束：$1 - \xi_i - y_i(w^Tx_i + b) \leq 0$ ; $-\xi_i \leq 0$</p>
</blockquote>
</li>
<li><p>将目标函数添加拉格朗日算子：<br>$$\begin{aligned}<br>&amp; L(w,b,\xi,\alpha,\lambda) = \frac {1}{2}{||w||<em>2}^2 + C\sum</em>{i=1}^m\xi_i + \sum_{i=1}^m\alpha_i[1 - \xi_i - y_i(w^Tx_i + b)] - \sum_{i=1}^m\lambda_i\xi_i \\<br>&amp; s.t. \ y_i(w^Tx_i + b) \geq 1 - \xi_i, \ C &gt; 0, \ \xi_i \geq 0, \ \alpha_i \geq 0, \ \lambda_i \geq 0, \ i \in \lbrace 1,2,…,m\rbrace<br>\end{aligned}$$</p>
</li>
<li><p>原优化问题 (primal problem)等价于：<br>$$p^* = \min_{w,b,\xi \geq 0} \ \max_{\alpha \geq 0, \lambda \geq 0} \ L(w,b,\xi,\alpha,\lambda)$$</p>
</li>
<li><p>原问题的对偶问题 (dual problem)为：<br>$$d^* = \max_{\alpha \geq 0, \lambda \geq 0} \ \min_{w,b,\xi \geq 0} \ L(w,b,\xi,\alpha,\lambda)$$</p>
</li>
<li><p>令 $\frac {\partial L}{\partial w} = 0$，可得：<br>$$\begin{aligned}<br>&amp; \frac {\partial L}{\partial w} = w - \sum_{i=1}^m\alpha_iy_ix_i = 0\\<br>&amp; w^* = \sum_{i=1}^m\alpha_iy_ix_i<br>\end{aligned}$$</p>
</li>
<li><p>令 $\frac {\partial L}{\partial b} = 0$，可得：<br>$$\begin{aligned}<br>&amp; \frac {\partial L}{\partial b} = -\sum_{i=1}^m\alpha_iy_i = 0 \\<br>&amp; \sum_{i=1}^m\alpha_iy_i = 0<br>\end{aligned}$$</p>
</li>
<li><p>令 $\frac {\partial L}{\partial \xi} = 0$，可得：<br>$$C - \alpha_i - \lambda_i = 0$$</p>
</li>
<li><p>将上述三个式子带入拉格朗日函数，消去 $w$ ，$b$ 和 $\xi$ ：<br>$$\begin{aligned}<br>&amp; L(\alpha,\lambda) = \frac {1}{2} \sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) + C\sum_{i=1}^m\xi_i + \sum_{i=1}^m\alpha_i - \sum_{i=1}^m\alpha_i\xi_i - \sum_{i=1}^m\alpha_iy_i[\sum_{j=1}^m(\alpha_jy_jx_j) \cdot x_i + b] - \sum_{i=1}^m\lambda_i\xi_i \\<br>&amp; \qquad \quad \ = -\frac {1}{2} \sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) + \sum_{i=1}^m\alpha_i + \sum_{i=1}^m\xi_i(C - \alpha_i - \lambda_i) \\<br>&amp; \qquad \quad \ = -\frac {1}{2} \sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) + \sum_{i=1}^m\alpha_i<br>\end{aligned}$$</p>
</li>
<li><p>原问题转为：<br>$$\begin{aligned}<br>&amp; \max_{\alpha,\lambda} \ -\frac {1}{2} \sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) + \sum_{i=1}^m\alpha_i \\<br>&amp; s.t. \ \sum_{i=1}^m\alpha_iy_i = 0, \ C - \alpha_i - \lambda_i = 0 \\<br>&amp; \qquad \alpha_i \geq 0, \ \lambda_i \geq 0, \ i \in \lbrace 1,2,…,m \rbrace<br>\end{aligned}$$</p>
<blockquote>
<p>由于 $\lambda_i \geq 0$ ，$C - \alpha_i - \lambda_i = 0$ 也可写为 $\alpha_i \leq C$</p>
</blockquote>
</li>
<li><p>将上面的式子加负号，变成最小化问题：<br>$$\begin{aligned}<br>&amp; \min_{\alpha,\lambda} \ \frac {1}{2} \sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) - \sum_{i=1}^m\alpha_i \\<br>&amp; s.t. \ \sum_{i=1}^m\alpha_iy_i = 0, \ 0 \leq \alpha_i \leq C, \ \lambda_i \geq 0, \ i \in \lbrace 1,2,…,m \rbrace<br>\end{aligned}$$</p>
</li>
<li><p>对KKT互补松弛条件的理解：<br>$$\begin{cases}<br>\alpha_i(1 - \xi_i - y_i(w^Tx_i + b)) = 0 \\<br>\lambda_i\xi_i = (C - \alpha_i)\xi_i = 0<br>\end{cases}$$    </p>
</li>
</ul>
<ol>
<li><p>若 $\alpha_i = 0$ ，则 $\xi_i = 0$ ，$y_i(w^Tx_i + b) \geq 1$</p>
<blockquote>
<p>$\alpha_i = 0$ 则对应样本不是支持向量；$\xi_i = 0$ 说明对应样本分类正确</p>
</blockquote>
</li>
<li><p>若 $\alpha_i = C$ ，则 $\xi_i \geq 0$ ，$y_i(w^Tx_i + b) \leq 1$</p>
<blockquote>
<p>若 $\xi_i = 0$ 则对应样本分类正确，若 $\xi_i &gt; 0$ 则对应样本偏离正确分类；$\alpha_i \neq 0$ 说明对应样本是支持向量，这一类称为边界支持向量 (bounded SV)</p>
</blockquote>
</li>
<li><p>若 $0 &lt; \alpha_i &lt; C$ ，则 $\xi_i = 0$ ，$y_i(w^Tx_i + b) = 1$</p>
<blockquote>
<p>$\xi_i = 0$ 说明对应样本分类正确；$\alpha_i \neq 0$ 说明对应样本是支持向量，这一类称为自由支持向量 (free SV)</p>
</blockquote>
</li>
</ol>
<h3 id="合页损失函数-Hinge-lost"><a href="#合页损失函数-Hinge-lost" class="headerlink" title="合页损失函数 (Hinge lost)"></a>合页损失函数 (Hinge lost)</h3><p>由 $y_i(x^Tx_i + b) \geq 1 - \xi_i$ 可得： $\xi_i \geq 1 - y_i(w^Tx_i + b)$ ；<br>又由于 $\xi_i \geq 0$ ，$\xi_i = max(0, 1 - y_i(w^Tx_i + b))$ ，称为合页损失函数</p>
<p><img src= "/img/loading.gif" data-src="https://s1.ax1x.com/2022/04/01/qh0f3Q.png" alt=""></p>
<p>合页损失函数的优点：</p>
<ul>
<li>凸函数，易优化</li>
<li>在自变量小于0的部分梯度较小，对错误的惩罚较轻</li>
<li>在自变量大于1的部分值为0，即：只要对某个数据分类正确，则不再针对该数据进行优化</li>
</ul>
<h2 id="Kernel-SVM"><a href="#Kernel-SVM" class="headerlink" title="Kernel SVM"></a>Kernel SVM</h2><p>通过添加松弛变量，可以将一定程度上线性不可分的数据使用SVM进行分类预测；而对于完全线性不可分的数据，可以通过增加数据的维度，将数据映射到高维空间，使其在高维空间中线性可分</p>
<p><img src= "/img/loading.gif" data-src="https://s1.ax1x.com/2022/04/01/qh0hcj.png" alt=""></p>
<p>例如：定义 $\phi(x) = x^2$ ，则将 $x_i$，$x_j$ 映射到 $\phi(x)$ 所在的空间后，目标函数 $\min_\alpha \ \frac {1}{2} \sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) - \sum_{i=1}^m\alpha_i$ 变为：<br>$$\min_\alpha \ \frac {1}{2} \sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j(\phi(x_i) \cdot \phi(x_j)) - \sum_{i=1}^m\alpha_i$$</p>
<p>如果直接将数据扩展到高维再计算内积，计算量非常大；且数据不可能扩展到无限维。因此使用 <strong>核技巧 (kernel trick)</strong> 解决该类问题</p>
<p>设计一个核函数 $K(x_i,x_j) = \phi(x_i) \cdot \phi(x_j)$，即数据 $x_i,x_j$ 在原始低维空间通过核函数 $K(x)$ 运算的结果等于数据 $x_i,x_j$ 映射到高维空间中的内积 $\phi(x_i) \cdot \phi(x_j)$ ，从而极大地降低计算的复杂度</p>
<ul>
<li><p>使用核函数的预测公式<br>$$\begin{aligned}<br>&amp; b = y_j - \sum_{i=1}^m\alpha_iy_iK(x_i,x_j) \\<br>&amp; \hat y = x^T \phi(x) + b = \sum_{i=1}^m \alpha_iy_iK(x_i,x) + b<br>\end{aligned}$$</p>
</li>
<li><p>成为核函数的条件：Mercer定理</p>
<p>  任何半正定的对称函数都可以作为核函数</p>
</li>
</ul>
<h3 id="常用的核函数"><a href="#常用的核函数" class="headerlink" title="常用的核函数"></a>常用的核函数</h3><ul>
<li><p>多项式核 (polynomial kernel)<br>$$K(x_i,x_j) = ((x_i \cdot x_j) + c)^d$$</p>
<blockquote>
<p>$c \geq 0$ 控制低阶项的强度；当 $c = 0, d = 1$ 时即为线性核 (linear kernel)，相当于没有核函数</p>
</blockquote>
</li>
<li><p>高斯核 (Gaussian kernel)也叫作Radial Basis Function (RBF) kernel<br>$$K(x_i,x_j) = exp(-\frac {||x_i - x_j||_2^2}{2\sigma^2})$$</p>
<blockquote>
<p>使用高斯核之前必须将数据进行标准化</p>
</blockquote>
</li>
<li><p>Sigmoid Kernel<br>$$K(x_i,x_j) = tanh(\alpha x_i^Tx_j + c)$$</p>
<blockquote>
<p>相当于一个没有隐藏层 (hidden layer)的简单神经网络</p>
</blockquote>
</li>
<li><p>cosine similarity Kernel<br>$$K(x_i,x_j) = \frac {x_i^Tx_j}{||x_i|| \ ||x_j||}$$</p>
<blockquote>
<p>常用于衡量两段文字的相似性；相当于两个向量的余弦相似度</p>
</blockquote>
</li>
<li><p>Chi-squared Kernel<br>$$K(x_i,x_j) = 1 - \sum_{i,j}^m \frac {(x_i - x_j)^2}{0.5(x_i + x_j)}$$</p>
<blockquote>
<p>常用于计算机视觉；衡量两个概率分布的相似性；输入数据必须是非负的，且必须使用了L1标准化 (L1 normalized)</p>
</blockquote>
</li>
</ul>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:zhuhaizhen1024@163.com">Zhu Haizhen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhuhaizhen.github.io/2020/11/29/SVM/">https://zhuhaizhen.github.io/2020/11/29/SVM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhuhaizhen.github.io" target="_blank">Haizhen's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/machine-learning/">machine learning</a><a class="post-meta__tags" href="/tags/SVM/">SVM</a></div><div class="post_share"><div class="social-share" data-image="https://s1.ax1x.com/2022/04/25/LoUEKP.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><button class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="post-qr-code__img" src="/image/wechat.jpg" alt="微信" onclick="window.open('/image/wechat.jpg')"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="post-qr-code__img" src="/image/alipay.jpg" alt="支付宝" onclick="window.open('/image/alipay.jpg')"/><div class="post-qr-code__desc">支付宝</div></li></ul></div></button></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/12/05/SMO/"><img class="prev-cover" data-src="https://s1.ax1x.com/2022/04/02/qo9ldO.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">SMO算法</div></div></a></div><div class="next-post pull-right"><a href="/2020/11/21/1121/"><img class="next-cover" data-src="https://s1.ax1x.com/2022/04/02/qo9Ei4.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">机器学习揭示微生物生态网络中丢失的边及潜在的相互作用机制</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/12/09/LP&simplex/" title="线性规划与单纯形算法"><img class="relatedPosts_cover" data-src="https://s1.ax1x.com/2022/04/02/qo9NQI.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="relatedPosts_title">线性规划与单纯形算法</div></div></a></div><div class="relatedPosts_item"><a href="/2021/01/04/PCA&LDA/" title="PCA和LDA"><img class="relatedPosts_cover" data-src="https://s1.ax1x.com/2022/04/02/qo9Ei4.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-01-04</div><div class="relatedPosts_title">PCA和LDA</div></div></a></div><div class="relatedPosts_item"><a href="/2020/12/05/SMO/" title="SMO算法"><img class="relatedPosts_cover" data-src="https://s1.ax1x.com/2022/04/02/qo9ldO.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-05</div><div class="relatedPosts_title">SMO算法</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var requestSetting = function (from,set) {
  var from = from
  var setting = set.split(',').filter(function(item){
  return from.indexOf(item) > -1
  });
  setting = setting.length == 0 ? from :setting;
  return setting
}

var guestInfo = requestSetting(['nick','mail','link'],'昵称,邮箱,网址')
var requiredFields = requestSetting(['nick','mail'],'昵称,邮箱')

window.valine = new Valine({
  el:'#vcomment',
  appId: 'yDcQjFWTIoApJ6qYc7DCVIjO-gzGzoHsz',
  appKey: 'y66ni3n3AtYNrLgaqdJGzAPo',
  placeholder: '请留下你的昵称和邮箱，方便接收回复',
  avatar: 'monsterid',
  meta: guestInfo,
  pageSize: '10',
  lang: 'zh-CN',
  recordIP: false,
  serverURLs: '',
  emojiCDN: '',
  emojiMaps: "",
  enableQQ: false,
  requiredFields: requiredFields
});</script></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Zhu Haizhen</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">君子不器</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fas fa-comments"></i></a><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script id="ribbon_piao" mobile="false" src="/js/third-party/piao.js"></script><script src="/js/third-party/activate-power-mode.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
document.body.addEventListener('input', POWERMODE);
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="/js/third-party/click_heart.js"></script><script src="/js/search/algolia.js"></script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>