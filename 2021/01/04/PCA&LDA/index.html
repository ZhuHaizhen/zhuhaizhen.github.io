<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>PCA和LDA | Haizhen's Blog</title><meta name="description" content="PCA主成分分析 (principal component analysis, PCA)是一种常用的无监督学习降维方法。其目的是在高维空间中找到一个超平面，使得所有样本点距离这个超平面的距离尽可能近 (最近重构性)且所有样本点在这个超平面上的投影都尽可能分开 (最大可分性)。主成分就是形成这个超平面的正交基向量，将原始数据投影到这个超平面上，即用主成分来表征原始数据，达到降维的目的。 向量内积与投"><meta name="keywords" content="machine learning,demensionality reduction"><meta name="author" content="Zhu Haizhen,zhuhaizhen1024@163.com"><meta name="copyright" content="Zhu Haizhen"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/image/favicon.png"><link rel="canonical" href="https://zhuhaizhen.github.io/2021/01/04/PCA&amp;LDA/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="PCA和LDA"><meta property="og:url" content="https://zhuhaizhen.github.io/2021/01/04/PCA&amp;LDA/"><meta property="og:site_name" content="Haizhen's Blog"><meta property="og:description" content="PCA主成分分析 (principal component analysis, PCA)是一种常用的无监督学习降维方法。其目的是在高维空间中找到一个超平面，使得所有样本点距离这个超平面的距离尽可能近 (最近重构性)且所有样本点在这个超平面上的投影都尽可能分开 (最大可分性)。主成分就是形成这个超平面的正交基向量，将原始数据投影到这个超平面上，即用主成分来表征原始数据，达到降维的目的。 向量内积与投"><meta property="og:image" content="https://s1.ax1x.com/2022/04/02/qo9QeK.png"><meta property="article:published_time" content="2021-01-04T10:19:29.000Z"><meta property="article:modified_time" content="2022-04-05T12:32:02.735Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="prev" title="集成学习方法" href="https://zhuhaizhen.github.io/2021/01/14/ensemble/"><link rel="next" title="逻辑回归" href="https://zhuhaizhen.github.io/2020/12/29/log-reg/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6b1c40d9cb0aeba42b229c550f73967b";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"ULAMU2VKLC","apiKey":"074084189662d00e3b579aef488f16a5","indexName":"hexo-blog","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: {"languages":{"author":"作者: Zhu Haizhen","link":"链接: ","source":"来源: Haizhen's Blog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/image/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">36</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">38</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/collection/"><i class="fa-fw fas fa-paw"></i><span> 随手记</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comments"></i><span> 留言板</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#PCA"><span class="toc-number">1.</span> <span class="toc-text">PCA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#向量内积与投影"><span class="toc-number">1.1.</span> <span class="toc-text">向量内积与投影</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#方差与协方差"><span class="toc-number">1.2.</span> <span class="toc-text">方差与协方差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PCA优化目标"><span class="toc-number">1.3.</span> <span class="toc-text">PCA优化目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PCA算法总结"><span class="toc-number">1.4.</span> <span class="toc-text">PCA算法总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LDA"><span class="toc-number">2.</span> <span class="toc-text">LDA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LDA算法推导"><span class="toc-number">2.1.</span> <span class="toc-text">LDA算法推导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LDA算法总结"><span class="toc-number">2.2.</span> <span class="toc-text">LDA算法总结</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://s1.ax1x.com/2022/04/02/qo9QeK.png)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Haizhen's Blog</a></span><span class="pull-right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/collection/"><i class="fa-fw fas fa-paw"></i><span> 随手记</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comments"></i><span> 留言板</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">PCA和LDA</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2021-01-04 18:19:29"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2021-01-04</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2022-04-05 20:32:02"><i class="fas fa-history fa-fw"></i> 更新于 2022-04-05</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/%E7%AE%97%E6%B3%95%E7%90%86%E8%A7%A3/">算法理解</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta__icon"></i><span>字数总计:</span><span class="word-count">1.9k</span><span class="post-meta__separator">|</span><i class="far fa-clock fa-fw post-meta__icon"></i><span>阅读时长: 7 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="far fa-comments fa-fw post-meta__icon"></i><span>评论数:</span><a href="/2021/01/04/PCA&amp;LDA/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/2021/01/04/PCA&amp;LDA/" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p>主成分分析 (principal component analysis, PCA)是一种常用的<strong>无监督学习</strong>降维方法。其目的是在高维空间中找到一个超平面，使得所有样本点距离这个超平面的距离尽可能近 (最近重构性)且所有样本点在这个超平面上的投影都尽可能分开 (最大可分性)。主成分就是形成这个超平面的正交基向量，将原始数据投影到这个超平面上，即用主成分来表征原始数据，达到降维的目的。</p>
<h3 id="向量内积与投影"><a href="#向量内积与投影" class="headerlink" title="向量内积与投影"></a>向量内积与投影</h3><ul>
<li><p>假设两个二维向量 $A = (x_1,y_1), \ B = (x_2,y_2)$ ，两个向量的夹角为 $a$ ，则向量 $A$ 在向量 $B$ 方向的投影为 $|A|\cos a$<br><img src= "/img/loading.gif" data-src="https://s1.ax1x.com/2022/04/01/q4Vyp4.png" alt=""></p>
</li>
<li><p>向量 $A$ 与向量 $B$ 的内积为 $A \cdot B = |A||B|\cos a$ 。<strong>若向量 $B$ 的模为1，即 $|B| = 1$ ，则向量 $A$ 在向量 $B$ 方向的投影等于向量 $A$ 与向量 $B$ 的内积</strong></p>
</li>
<li><p>根据向量的模的定义 $|A| = \sqrt{x_1^2 + y_1^2}$ ，对于任何一个向量，只要<strong>将其各个分量都除以该向量的模</strong>，就得到了与该向量方向相同，且模为1的基向量</p>
</li>
<li><p>对于M个N维向量，若要将其投影到R个N维向量组成的超平面，首先将R个基向量按行排列成左侧矩阵，将M个向量按列排列成右侧矩阵，两个矩阵的乘积就是投影之后的结果<br>$$\begin{pmatrix}-p_1- \\ -p_2- \\ \vdots \\ -p_R-\end{pmatrix} \begin{pmatrix}| &amp; | &amp; \cdots &amp; | \\a_1 &amp; a_2 &amp; \cdots &amp; a_M \\ | &amp; | &amp; \cdots &amp; |\end{pmatrix} = \begin{pmatrix}p_1a_1 &amp; p_1a_2 &amp; \cdots &amp; p_1a_M \\ p_2a_1 &amp; p_2a_2 &amp; \cdots &amp; p_2a_M \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ p_Ra_1 &amp; p_Ra_2 &amp; \cdots &amp; p_Ra_M\end{pmatrix}$$</p>
<blockquote>
<p>左边的矩阵的维度为 $(R,N)$ ，右边矩阵的维度为 $(N,M)$ 相乘后矩阵的维度为 $(R,M)$ 。两个矩阵相乘的物理意义是将右边矩阵中的每一列列向量变换到以左边矩阵中每一行行向量为基的空间中去</p>
</blockquote>
</li>
</ul>
<h3 id="方差与协方差"><a href="#方差与协方差" class="headerlink" title="方差与协方差"></a>方差与协方差</h3><ul>
<li><p>根据最大可分性的目标，我们希望投影后各个样本点尽量分散，这种分散程度可以用方差表示：<br>$$Var(a) = \frac{1}{m}\sum_{i=1}^m(a_i - \mu)^2; \quad \mu = \frac{1}{m}\sum_{i=1}^m a_i$$</p>
</li>
<li><p>为了方便计算，首先将数据进行<strong>去中心化</strong>，即把每一个元素都减去均值，使得去中心化后的所有元素均值为0，此时方差表示为：<br>$$Var(a) = \frac{1}{m}\sum_{i=1}^m a_i^2$$</p>
</li>
<li><p>对于二维降维成一维的问题，只要找到使方差最大的方向即可。对于更高维的降维问题，在选择其它投影方向时，为了保留更多的原始信息，应该尽可能保证各投影方向之间不存在相关性。用协方差表示向量之间的相关性，若两个向量正交，协方差为0。对于去中心化的向量，其协方差为：<br>$$Cov(a,b) = \frac{1}{m}\sum_{i=1}^m a_ib_i$$</p>
</li>
<li><p>矩阵 $X$ 的协方差矩阵可表示为：<br>$$C = \frac{1}{m}XX^T$$</p>
</li>
</ul>
<h3 id="PCA优化目标"><a href="#PCA优化目标" class="headerlink" title="PCA优化目标"></a>PCA优化目标</h3><ul>
<li><p>假设我们找到一个单位基向量 $u_1$ ，将去中心化的 $x_i$ 在 $u_1$ 方向上进行投影，则投影后的向量为 $u_1^Tx_i$</p>
</li>
<li><p>投影后的向量均值为：<br>$$\frac{1}{m}\sum_{i=1}^m u_1^Tx_i = u_1^T\frac{1}{m}\sum_{i=1}^m x_i = 0$$</p>
</li>
<li><p>投影后的方差为：<br>$$\begin{align}<br>\frac{1}{m}\sum_{i=1}^m (u_1^Tx_i)^2 &amp; = \frac{1}{m}\sum_{i=1}^m u_1^Tx_ix_i^Tu_1 \\<br>&amp; = u_1^T\frac{1}{m}\sum_{i=1}^m x_ix_i^Tu_1 \\<br>&amp; = u_1^T C u_1 \\<br>C = \frac{1}{m}XX^T<br>\end{align}$$</p>
</li>
<li><p>PCA的优化目标为：<br>$$\max_{u_1} \ u_1^T C u_1 \qquad s.t. ||u_1||_2^2 = 1$$</p>
</li>
<li><p>将目标函数添加拉格朗日算子：<br>$$L = u_1^TCu_1 + \lambda(1 - ||u_1||^2_2)$$</p>
</li>
<li><p>对 $u_1$ 求导，令导数为0：<br>$$\begin{align}<br>&amp; \frac{\partial L}{\partial u_1} = 2u_1 C + \lambda (-2u_1) = 0 \\<br>&amp; \implies Cu_1 = \lambda u_1<br>\end{align}$$</p>
<blockquote>
<p>$\lambda$ 和 $u_1$ 分别为协方差矩阵 $C$ 的特征值和特征向量</p>
</blockquote>
</li>
<li><p>推广到多维超平面可得，目标函数 $U^TCU = U\lambda U$ 。由于 $U$ 由一系列单位基向量组成， $U^TU = 1$ ， $U^TCU = U^T\lambda U = \lambda$ 。<strong>协方差矩阵的特征值等于矩阵 $X$ 投影后的方差</strong>，要想最大化方差，投影方向应该选择协方差矩阵最大特征值对应的特征向量</p>
</li>
<li><p>将协方差矩阵的特征值由大到小排列，提取对应的前K行特征向量组成矩阵 $(u_1,u_2,\cdots,u_k)$ 乘以去中心化的矩阵 $X$ ，就得到了降维后的矩阵</p>
</li>
</ul>
<h3 id="PCA算法总结"><a href="#PCA算法总结" class="headerlink" title="PCA算法总结"></a>PCA算法总结</h3><p>对于m条包含n个特征的数据：</p>
<ol>
<li><p>将数据按列组成n行m列的原始矩阵</p>
</li>
<li><p>将原始矩阵中的每个元素减去所在行各元素的均值，得到去中心化的矩阵 $X$</p>
</li>
<li><p>求出协方差矩阵 $C = \frac{1}{m}XX^T$</p>
</li>
<li><p>求出协方差矩阵的特征值及对应的特征向量</p>
</li>
<li><p>将特征值由大到小排列，提取对应的前K行特征向量组成矩阵 $P$</p>
</li>
<li><p>$Y = PX$ 即为降到K维后的数据</p>
</li>
</ol>
<h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><p>线性判别分析 (linear discriminant analysis, LDA)是一种<strong>监督学习</strong>的算法。其核心思想是将高维的数据投影到一条直线上，是的同类别的数据投影点尽可能接近，不同类别的数据投影点尽可能远离<br><img src= "/img/loading.gif" data-src="https://s1.ax1x.com/2022/04/01/q4V61J.png" alt=""></p>
<h3 id="LDA算法推导"><a href="#LDA算法推导" class="headerlink" title="LDA算法推导"></a>LDA算法推导</h3><ul>
<li><p>以二分类为例，假设有样本 $C_1, C_2$ ，其均值分别为 $\mu_1, \mu_2$ ，投影方向为 $w$ 。则投影后两个样本之间的距离为：<br>$$D(C_1, C_2) = ||w^T(\mu_1 - \mu_2)||^2_2$$</p>
</li>
<li><p>投影后两个样本的方差分别为：<br>$$\begin{align}<br>&amp; Var(C_1^{\prime}) = \sum_{x \in C_1}(w^Tx - w^T\mu_1)^2 \\<br>&amp; Var(C_2^{\prime}) = \sum_{x \in C_2}(w^Tx - w^T\mu_2)^2<br>\end{align}$$</p>
</li>
<li><p>根据最大化类间距离和最小化类内距离的目标，可以将目标函数表示为：<br>$$\begin{align}<br>\max_w J(w) &amp; = \frac{D(C_1, C_2)}{Var(C_1^{\prime}) + Var(C_2^{\prime})} \\<br>&amp; = \frac{w^T(\mu_1 - \mu_2)(\mu_1 - \mu_2)^Tw}{\sum_{x \in C_i}w^T(x - \mu_i)(x - \mu_i)^Tw}<br>\end{align}$$</p>
</li>
<li><p>定义类内散度矩阵 $S_w$ ：<br>$$S_w = \sum_{x \in C_i}(x - \mu_i)(x - \mu_i)^T$$</p>
</li>
<li><p>定义类间散度矩阵 $S_b$ ：<br>$$S_b = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T$$</p>
</li>
<li><p>将目标函数简化为：<br>$$J(w) = \frac{w^TS_bw}{w^TS_ww}$$</p>
</li>
<li><p>对 $J(w)$ 求 $w$ 的偏导数，令导数为0：<br>$$\begin{align}<br>\frac{\partial J}{\partial w} &amp; = 2S_bw(w^Ts_ww)^{-1} - 2S_ww(w^TS_bw)(w^TS_ww)^{-2} \\<br>&amp; = 2S_bw - 2S_ww(w^TS_bw)(w^TS_ww)^{-1} = 0 \\<br>&amp; \implies (w^TS_ww)S_bw = (w^TS_bw)S_ww \\<br>&amp; \implies \frac{w^TS_bw}{w^TS_ww} = \frac{S_bw}{S_ww}<br>\end{align}$$</p>
</li>
<li><p>将上式带回目标函数可得：<br>$$\begin{align}<br>&amp; J(w) = \frac{Sbw}{S_ww} \\<br>&amp; S_bw = J(w)S_ww \\<br>&amp; S_w^{-1}S_bw = J(w)w<br>\end{align}$$</p>
<blockquote>
<p>$J(w)$ 和 $w$ 分别对应了矩阵 $S_w^{-1}S_b$ 的特征值和特征向量，要想最大化目标函数，只需要求得矩阵 $S_w^{-1}S_b$ 的最大特征向量，投影方向为该特征值对应的特征向量</p>
</blockquote>
</li>
<li><p>扩展到多类高维，定义全局散度矩阵 $S_t$ :<br>$$S_t = S_b + S_w = \sum_{i=1}^m(x_i - \mu)(x_i - \mu)^T$$</p>
<blockquote>
<p>$\mu$ 为全局中心， $N$ 为类别数， $m$ 为样本数</p>
</blockquote>
</li>
<li><p>此时，类内散度矩阵定义为每个类的类内散度矩阵之和：<br>$$S_w = \sum_{i=1}^NS_{wi}$$</p>
</li>
<li><p>可以推导出类间散度矩阵的定义：<br>$$S_b = \sum_{i=1}^Nm_i(\mu_i - \mu)(\mu_i - \mu)^T$$</p>
</li>
</ul>
<h3 id="LDA算法总结"><a href="#LDA算法总结" class="headerlink" title="LDA算法总结"></a>LDA算法总结</h3><ol>
<li><p>计算每个类别的均值 $\mu_i$ 和全局中心 $\mu$</p>
</li>
<li><p>计算全局散度矩阵 $S_t$ ，类内散度矩阵 $S_w$ 和类间散度矩阵 $S_b$</p>
</li>
<li><p>求矩阵 $S_w^{-1}S_b$ 的特征值和特征向量</p>
</li>
<li><p>将矩阵 $S_w^{-1}S_b$ 的特征值从大到小排列，取前K个特征值对应的特征向量</p>
</li>
<li><p>计算投影矩阵</p>
</li>
</ol>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:zhuhaizhen1024@163.com">Zhu Haizhen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhuhaizhen.github.io/2021/01/04/PCA&amp;LDA/">https://zhuhaizhen.github.io/2021/01/04/PCA&amp;LDA/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhuhaizhen.github.io" target="_blank">Haizhen's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/machine-learning/">machine learning</a><a class="post-meta__tags" href="/tags/demensionality-reduction/">demensionality reduction</a></div><div class="post_share"><div class="social-share" data-image="https://s1.ax1x.com/2022/04/02/qo9ZW9.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><button class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="post-qr-code__img" src="/image/wechat.jpg" alt="微信" onclick="window.open('/image/wechat.jpg')"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="post-qr-code__img" src="/image/alipay.jpg" alt="支付宝" onclick="window.open('/image/alipay.jpg')"/><div class="post-qr-code__desc">支付宝</div></li></ul></div></button></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/01/14/ensemble/"><img class="prev-cover" data-src="https://s1.ax1x.com/2022/04/02/qo9ldO.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">集成学习方法</div></div></a></div><div class="next-post pull-right"><a href="/2020/12/29/log-reg/"><img class="next-cover" data-src="https://s1.ax1x.com/2022/04/02/qo9Ei4.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">逻辑回归</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/12/09/LP&simplex/" title="线性规划与单纯形算法"><img class="relatedPosts_cover" data-src="https://s1.ax1x.com/2022/04/02/qo9ZW9.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="relatedPosts_title">线性规划与单纯形算法</div></div></a></div><div class="relatedPosts_item"><a href="/2020/12/05/SMO/" title="SMO算法"><img class="relatedPosts_cover" data-src="https://s1.ax1x.com/2022/04/02/qo9Ei4.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-05</div><div class="relatedPosts_title">SMO算法</div></div></a></div><div class="relatedPosts_item"><a href="/2020/11/29/SVM/" title="SVM算法"><img class="relatedPosts_cover" data-src="https://s1.ax1x.com/2022/04/02/qo9QeK.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-29</div><div class="relatedPosts_title">SVM算法</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var requestSetting = function (from,set) {
  var from = from
  var setting = set.split(',').filter(function(item){
  return from.indexOf(item) > -1
  });
  setting = setting.length == 0 ? from :setting;
  return setting
}

var guestInfo = requestSetting(['nick','mail','link'],'昵称,邮箱,网址')
var requiredFields = requestSetting(['nick','mail'],'昵称,邮箱')

window.valine = new Valine({
  el:'#vcomment',
  appId: 'yDcQjFWTIoApJ6qYc7DCVIjO-gzGzoHsz',
  appKey: 'y66ni3n3AtYNrLgaqdJGzAPo',
  placeholder: '请留下你的昵称和邮箱，方便接收回复',
  avatar: 'monsterid',
  meta: guestInfo,
  pageSize: '10',
  lang: 'zh-CN',
  recordIP: false,
  serverURLs: '',
  emojiCDN: '',
  emojiMaps: "",
  enableQQ: false,
  requiredFields: requiredFields
});</script></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Zhu Haizhen</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">君子不器</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fas fa-comments"></i></a><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script id="ribbon_piao" mobile="false" src="/js/third-party/piao.js"></script><script src="/js/third-party/activate-power-mode.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
document.body.addEventListener('input', POWERMODE);
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="/js/third-party/click_heart.js"></script><script src="/js/search/algolia.js"></script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>